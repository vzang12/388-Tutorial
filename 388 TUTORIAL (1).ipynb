{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce you to the basics of the decision tree model, how to implement a classification decision tree, and how decision trees are used in real world applications, specifically in health and diagnosis of illnesses. Given some specific details about a person, can we predict if they have some illness, for example cancer or the flu? Many illnesses will have a list of symptoms where if you were to be experiencing some of them, you could have a high chance of having that illness. And so, decision trees are a great tool that will not only help make these predictions based on certain details, but also they provide a great visual that is very easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a tree diagram where at each node, a decision can be made such that you follow the branches corresponding to which decision you made, and ultimately ending up at a leaf node with some possible outcome to the problem. At each internal node of a decision tree, there will be one value attribute, $x_i$, which can either be a discrete value or a continuous value. Each branch from a node selects one value for $x_i$, and each leaf node predicts the outcome $Y$. There are two main types of decision trees: classification trees and regression trees. In this tutorial, we will be working with classification decision trees which means that our outcome prediction is a discrete value. The regression tree predicts outcomes that can be considered real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be focusing on implementing a classficiation decision tree in Python to decide given a person's symptoms, whether they have COVID or not. In this dataset, we will be working with discrete attributes as well as a discrete outcome. We will be using a dataset from Kaggle that includes symptoms as the attributes and COVID presence as the outcome: https://www.kaggle.com/hemanthhari/symptoms-and-covid-presence\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "- libraries\n",
    "- data structure for decision tree\n",
    "- general algorithm for decision tree\n",
    "- loading the data\n",
    "- entropy and information gain calculations\n",
    "- building the decision tree\n",
    "- making predictions\n",
    "- error rate and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following libraries in order to implement our decision tree. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are implementing a decision tree, it makes the most sense to represent our decision tree using an actual tree data structure. We will define a node class with different attributes that will help facilitate the implementation for our decision tree and then build a tree with these node objects.\n",
    "- self.left: this will be the left child of the node\n",
    "- self.right: this will be the right child of the node\n",
    "- self.data: this contains the current dataset we are working with\n",
    "- self.attribute: the attribute that is split on at the current node\n",
    "- self.colIndex: this is the column index of the attribute in our data table\n",
    "- self.maxDepth: how deep we want our tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, depth):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = data\n",
    "        self.attribute = None\n",
    "        self.colIndex = None\n",
    "        self.maxDepth = depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Algorithm for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a general outline for how to build a decision tree:\n",
    "\n",
    "1. Start with an empty tree at the root, which is just an empty node and your training dataset\n",
    "2. Do the following until some stopping criterion is met:\n",
    "       a) pick the best attribute to put at the node, let this attribute be called A\n",
    "       b) assign A as the decision attribute for the current empty node you are at\n",
    "       c) for each value of A, create a new descendent of node (the branches)\n",
    "       d) sort training examples to leaf nodes\n",
    "       e) repeat loop at leaf nodes\n",
    "3. Output decision tree\n",
    "\n",
    "This tutorial will walk us through the above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is download and load our data into some form that we can use easily when implementing our decision tree. You are going to click Download from this Kaggle webpage: https://www.kaggle.com/hemanthhari/symptoms-and-covid-presence (note: you will need an account in order to download datasets from Kaggle). Then, unzip the archieve.zip file to create an archive folder which will include a file named Covid dataset in the form of a CSV file. So then in order for us to use this dataset, we need the Covid dataset file to be in the the same folder as this notebook, and then we can load the data by using the following commands: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breathing Problem</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Dry Cough</th>\n",
       "      <th>Sore throat</th>\n",
       "      <th>Running Nose</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>Chronic Lung Disease</th>\n",
       "      <th>Headache</th>\n",
       "      <th>Heart Disease</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>Fatigue</th>\n",
       "      <th>Gastrointestinal</th>\n",
       "      <th>Abroad travel</th>\n",
       "      <th>Contact with COVID Patient</th>\n",
       "      <th>Attended Large Gathering</th>\n",
       "      <th>Visited Public Exposed Places</th>\n",
       "      <th>Family working in Public Exposed Places</th>\n",
       "      <th>Wearing Masks</th>\n",
       "      <th>Sanitization from Market</th>\n",
       "      <th>COVID-19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Breathing Problem Fever Dry Cough Sore throat Running Nose Asthma  \\\n",
       "0               Yes   Yes       Yes         Yes          Yes     No   \n",
       "1               Yes   Yes       Yes         Yes           No    Yes   \n",
       "2               Yes   Yes       Yes         Yes          Yes    Yes   \n",
       "3               Yes   Yes       Yes          No           No    Yes   \n",
       "4               Yes   Yes       Yes         Yes          Yes     No   \n",
       "\n",
       "  Chronic Lung Disease Headache Heart Disease Diabetes  ... Fatigue   \\\n",
       "0                   No       No            No      Yes  ...      Yes   \n",
       "1                  Yes      Yes            No       No  ...      Yes   \n",
       "2                  Yes      Yes            No      Yes  ...      Yes   \n",
       "3                   No       No           Yes      Yes  ...       No   \n",
       "4                  Yes      Yes           Yes      Yes  ...       No   \n",
       "\n",
       "  Gastrointestinal  Abroad travel Contact with COVID Patient  \\\n",
       "0               Yes            No                        Yes   \n",
       "1                No            No                         No   \n",
       "2               Yes           Yes                         No   \n",
       "3                No           Yes                         No   \n",
       "4               Yes            No                        Yes   \n",
       "\n",
       "  Attended Large Gathering Visited Public Exposed Places  \\\n",
       "0                       No                           Yes   \n",
       "1                      Yes                           Yes   \n",
       "2                       No                            No   \n",
       "3                      Yes                           Yes   \n",
       "4                       No                           Yes   \n",
       "\n",
       "  Family working in Public Exposed Places Wearing Masks  \\\n",
       "0                                     Yes            No   \n",
       "1                                      No            No   \n",
       "2                                      No            No   \n",
       "3                                      No            No   \n",
       "4                                      No            No   \n",
       "\n",
       "  Sanitization from Market COVID-19  \n",
       "0                       No      Yes  \n",
       "1                       No      Yes  \n",
       "2                       No      Yes  \n",
       "3                       No      Yes  \n",
       "4                       No      Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"Covid_Dataset.csv\", delimiter=\",\", quotechar='\"')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can move any further with our implementation of the decision tree, we need to decide how to choose the \"best\" attribute at each node. For this, we need something called entropy and information gain. Entropy measures the impurity of a collection of examples (how separable the data are). For a decision tree, we want a lower entropy so that our data is more pure. <br>\n",
    "The entropy H(Y) for a random variable Y with n possible values is: <br>\n",
    "$H(Y) = - \\sum \\limits _{i=1} ^{n} P(Y = i)log_2P(Y=i)$ <br>\n",
    "So then to decide which attribute decreases the most entropy for the dataset we use information gain, also called mutual information. Information gain, $I_S(Y, A)$ is the reduction in entropy of target variable Y for data sample S, due to sorting on a variable A. So when looking at different attributes, we want the attribute that gives us the highest information gain, resulting in the largest decrease to the entropy of our whole dataset. In order to calculate information gain, we will need additional formulas. <br>\n",
    "Specific conditional entropy H(Y|X = v) of Y given X = v: <br>\n",
    "$H(Y|X = v) = 0 \\sum \\limits _{i=1} ^{n} P(Y = i|X = v)log_2P(Y = i|X = v)$ <br>\n",
    "Conditional entropy H(Y|X) of Y given X: <br>\n",
    "$H(Y|X) = \\sum \\limits _{v \\in values(X)} ^{} P(X = v)H(Y|X = v)$ <br>\n",
    "So then information gain, $I(Y, X)$, is calculated by: <br>\n",
    "$I(Y, X) = H(Y) - H(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the formulas needed to find the best attribute at some node, let's implement them into code! First, let's implement a function that just calculates the total entropy of the dataset, H(Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalEntropyCalc(root):\n",
    "    y_yes = root.data.loc[root.data['COVID-19'] == 'Yes']\n",
    "    y_no = root.data.loc[root.data['COVID-19'] == 'No']\n",
    "    count1 = len(y_yes)\n",
    "    count2 = len(y_no)\n",
    "    total = count1+count2\n",
    "    if (count1==0 and count2 == 0): return 0\n",
    "    if(count1 ==0):\n",
    "        entropy = (count2/float(total))*np.log2(count2/float(total))\n",
    "        return (-1*entropy)\n",
    "    if(count2==0):\n",
    "        entropy = (count1/float(total))*np.log2(count1/float(total))\n",
    "        return (-1*entropy)\n",
    "    entropy = (count1/float(total))*np.log2(count1/float(total)) + (count2/float(total))*np.log2(count2/float(total))\n",
    "    return (-1*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need a function that partitions our data based on some attribute A so we can determine the proportion of people with COVID given that they either have attribute A or don't have attribute A and the proportion of people who don't have COVID given that they either have attribute A or don't have attribute A. These numbers will be needed to calculate the specific conditional entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(colAttribute, root):\n",
    "    x_yes = root.data.loc[root.data[colAttribute] == 'Yes']\n",
    "    x_no = root.data.loc[root.data[colAttribute] == 'No']\n",
    "    xYes_yYes = x_yes.loc[x_yes['COVID-19'] == 'Yes']\n",
    "    xYes_yNo = x_yes.loc[x_yes['COVID-19'] == 'No']\n",
    "    xNo_yYes = x_no.loc[x_no['COVID-19'] == 'Yes']\n",
    "    xNo_yNo = x_no.loc[x_no['COVID-19'] == 'No']\n",
    "    return (x_yes, xYes_yYes, xYes_yNo, x_no, xNo_yYes, xNo_yNo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have partitioned our data given some attribute A, we can now calculate the conditional entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyCalc(y_yes, y_no):\n",
    "    count1 = len(y_yes)\n",
    "    count2 = len(y_no)\n",
    "    if (count1 == 0 and count2 == 0): return 0\n",
    "    total = count1+count2\n",
    "    if (count1 == 0):\n",
    "        entropy = entropy = (count2/float(total))*np.log2(count2/float(total))\n",
    "        return (-1*entropy)\n",
    "    if(count2 == 0):\n",
    "        entropy = (count1/float(total))*np.log2(count1/float(total))\n",
    "        return (-1*entropy)\n",
    "    entropy = (count1/float(total))*np.log2(count1/float(total)) + (count2/float(total))*np.log2(count2/float(total))\n",
    "    return (-1*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally, we can implement a function to calculate the information gain for some attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInfoCalc(totalEntropy, entropy1, x_yes, entropy2, x_no):\n",
    "    count1 = len(x_yes)\n",
    "    count2 = len(x_no)\n",
    "    total = count1+count2\n",
    "    infoGain = totalEntropy - ( (count1/float(total))*entropy1 + (count2/float(total))*entropy2 )\n",
    "    return infoGain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the way to find the best attribute to split at for each node, we will implement another function to help with printing out our decision tree at the end. It is very useful to see the actual proportions for the data at each node, so we will keep count of the majority portion and the minority portion of data at every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityVote(root):\n",
    "    y_yes = root.data.loc[root.data['COVID-19'] == 'Yes']\n",
    "    y_no = root.data.loc[root.data['COVID-19'] == 'No']\n",
    "    count_yes = len(y_yes)\n",
    "    count_no = len(y_no)\n",
    "    \n",
    "    if (count_yes > count_no):\n",
    "        majority = 'Yes'\n",
    "        minority = 'No'\n",
    "        return(majority, count_yes, minority, count_no)\n",
    "    else:\n",
    "        majority = 'No'\n",
    "        minority = 'Yes'\n",
    "        return(majority, count_no, minority, count_yes)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function before we get to building our tree is to help split the data based on a single attribute. So once we find the best attribute, we need to split our data based on the whether a person is 'yes' for attribute A or 'no'. So essentially we are going to use this function to build a decision stump (which is a decision tree with depth 1) at every node, just so we can separate the building of our full decision tree into smaller, and more clear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionStump(colAttribute, root):\n",
    "    x_yes = root.data.loc[root.data[colAttribute] == 'Yes']\n",
    "    x_no = root.data.loc[root.data[colAttribute] == 'No']\n",
    "    count_yes = len(x_yes)\n",
    "    count_no = len(x_no)\n",
    "    if(count_yes > count_no):\n",
    "        return(x_yes, x_no)\n",
    "    else:\n",
    "        return(x_no, x_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have all of our functions with attribute choosing and printing the tree, let's start building the tree! First, we must initialize our tree to be an empty node, containing the entire dataset. Here, we are initializing the max depth of our tree to be 1 just to demonstrate splitting on one attribute, but later we will look at the affects of changing max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Node(dataframe, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the main loop for the decision tree algorithm. We will want to calculate the information gain of every attribute, find the largest one, and then split our data accordingly. We continue this process until some stopping criteria is met. There are three base cases. One stopping criteria is when we have reached our max depth. The max depth is a hyperparameter, which means that it is not derived through the learning process, but rather chosen outside of the model and can help make the model a better fit for the data and a better predictor. Another stopping criteria is when the total entropy of the dataset is zero. This means that our data is perfectly classified and so there is no need to continue splitting. The last base case to consider is when all the information gains are zero. This indicates that we have already used all the attributes to split on and there are no more attributes to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treeBuilder(root, depth, attributes, printBool):\n",
    "    #Base Case: Max depth reached\n",
    "    if (depth >= root.maxDepth):\n",
    "        return (root)\n",
    "    totalEntropy = totalEntropyCalc(root)\n",
    "    #Base Case: Data is perfectly classified\n",
    "    if (totalEntropy == 0):\n",
    "        return (root)\n",
    "    infoGains = []\n",
    "    for attribute in attributes:\n",
    "        (x_yes, xYes_yYes, xYes_yNo, x_no, xNo_yYes, xNo_yNo) = partition(attribute, root)\n",
    "        entropy1 = entropyCalc(xYes_yYes, xYes_yNo)\n",
    "        entropy2 = entropyCalc(xNo_yYes, xNo_yNo)\n",
    "        infoGain = mutualInfoCalc(totalEntropy, entropy1, x_yes, entropy2, x_no)\n",
    "        infoGains = infoGains + [infoGain]\n",
    "    #Base Case: all attributes have been used\n",
    "    if (sum(infoGains) == 0):\n",
    "        return (root)\n",
    "    #Recursive Case:\n",
    "    else:\n",
    "        if (depth == 0):\n",
    "            (major, majorVote, minor, minorVote) = majorityVote(root)\n",
    "            if printBool:\n",
    "                print(\"[\" + str(majorVote) + \" \" + major + \"/\" + str(minorVote) + \" \" +  minor + \"]\")\n",
    "        maxIG = -1\n",
    "        splitCol = -1\n",
    "        #find the attribute that gives the highest information gain\n",
    "        for i in range(len(infoGains)):\n",
    "            if infoGains[i] > maxIG:\n",
    "                splitCol = i\n",
    "                maxIG = infoGains[i]\n",
    "        #split on best attribute\n",
    "        root.attribute = attributes[splitCol]\n",
    "        root.splitCol = splitCol\n",
    "        root.colIndex = splitCol\n",
    "        (leftData, rightData) = decisionStump(attributes[splitCol], root)\n",
    "        if (len(leftData) < len(rightData)):\n",
    "            root.right = Node(leftData, root.maxDepth)\n",
    "            root.left = Node(rightData, root.maxDepth)\n",
    "        else:\n",
    "            root.left = Node(leftData, root.maxDepth)\n",
    "            root.right = Node(rightData, root.maxDepth)\n",
    "        (mL, mVotesL, minL, minVotesL) = majorityVote(root.left)\n",
    "        if printBool:\n",
    "            print(\" | \" * (depth+1), root.attribute + \" = \" + root.left.data.iloc[0][splitCol] + \": [\" + str(mVotesL) + \" \" + mL + \"/\" + str(minVotesL) + \" \" + minL + \"]\")\n",
    "        treeBuilder(root.left, depth+1, attributes, printBool)\n",
    "        (mR, mVotesR, minR, minVotesR) = majorityVote(root.right)\n",
    "        if printBool:\n",
    "            print(\" | \" * (depth+1), root.attribute + \" = \" + root.right.data.iloc[0][splitCol] + \": [\" + str(mVotesR) + \" \" + mR + \"/\" + str(minVotesR) + \" \" + minR + \"]\")\n",
    "        treeBuilder(root.right, depth+1, attributes, printBool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tree builder function, let's build our first decision tree with a depth of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4383 Yes/1051 No]\n",
      " |  Abroad travel = No: [1932 Yes/1051 No]\n",
      " |  Abroad travel = Yes: [2451 Yes/0 No]\n"
     ]
    }
   ],
   "source": [
    "colAttributes = list(dataframe.columns)\n",
    "colAttributes = colAttributes[0 : (len(colAttributes)-1)]\n",
    "decision_tree = treeBuilder(root, 0, colAttributes, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our decision tree, we can see that the best attribute to split on is 'Abroad travel' because it leads to the most pure dataset. Now let's see what our tree looks like with a larger max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4383 Yes/1051 No]\n",
      " |  Abroad travel = No: [1932 Yes/1051 No]\n",
      " |  |  Sore throat = Yes: [1638 Yes/284 No]\n",
      " |  |  |  Attended Large Gathering = Yes: [977 Yes/0 No]\n",
      " |  |  |  Attended Large Gathering = No: [661 Yes/284 No]\n",
      " |  |  |  |  Contact with COVID Patient = No: [277 Yes/252 No]\n",
      " |  |  |  |  |  Breathing Problem = Yes: [232 Yes/92 No]\n",
      " |  |  |  |  |  |  Fever = Yes: [212 Yes/49 No]\n",
      " |  |  |  |  |  |  |  Family working in Public Exposed Places = No: [112 Yes/49 No]\n",
      " |  |  |  |  |  |  |  |  Asthma = Yes: [54 Yes/38 No]\n",
      " |  |  |  |  |  |  |  |  |  Headache = No: [38 No/33 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Fatigue  = Yes: [38 No/28 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Fatigue  = No: [5 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  |  Headache = Yes: [21 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  Asthma = No: [58 Yes/11 No]\n",
      " |  |  |  |  |  |  |  |  |  Heart Disease = No: [37 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  |  Heart Disease = Yes: [21 Yes/11 No]\n",
      " |  |  |  |  |  |  |  |  |  |  Dry Cough = Yes: [16 Yes/11 No]\n",
      " |  |  |  |  |  |  |  |  |  |  Dry Cough = No: [5 Yes/0 No]\n",
      " |  |  |  |  |  |  |  Family working in Public Exposed Places = Yes: [100 Yes/0 No]\n",
      " |  |  |  |  |  |  Fever = No: [43 No/20 Yes]\n",
      " |  |  |  |  |  |  |  Asthma = No: [43 No/12 Yes]\n",
      " |  |  |  |  |  |  |  |  Dry Cough = Yes: [27 No/12 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Chronic Lung Disease = Yes: [16 No/4 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Chronic Lung Disease = No: [11 No/8 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Headache = Yes: [11 No/4 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Headache = No: [4 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  Dry Cough = No: [16 No/0 Yes]\n",
      " |  |  |  |  |  |  |  Asthma = Yes: [8 Yes/0 No]\n",
      " |  |  |  |  |  Breathing Problem = No: [160 No/45 Yes]\n",
      " |  |  |  |  |  |  Dry Cough = No: [106 No/0 Yes]\n",
      " |  |  |  |  |  |  Dry Cough = Yes: [54 No/45 Yes]\n",
      " |  |  |  |  |  |  |  Fever = No: [54 No/0 Yes]\n",
      " |  |  |  |  |  |  |  Fever = Yes: [45 Yes/0 No]\n",
      " |  |  |  |  Contact with COVID Patient = Yes: [384 Yes/32 No]\n",
      " |  |  |  |  |  Chronic Lung Disease = Yes: [222 Yes/0 No]\n",
      " |  |  |  |  |  Chronic Lung Disease = No: [162 Yes/32 No]\n",
      " |  |  |  |  |  |  Family working in Public Exposed Places = Yes: [104 Yes/0 No]\n",
      " |  |  |  |  |  |  Family working in Public Exposed Places = No: [58 Yes/32 No]\n",
      " |  |  |  |  |  |  |  Fatigue  = Yes: [35 Yes/32 No]\n",
      " |  |  |  |  |  |  |  |  Breathing Problem = Yes: [32 No/26 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Running Nose = Yes: [32 No/21 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Dry Cough = Yes: [16 No/16 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Dry Cough = No: [16 No/5 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Running Nose = No: [5 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  Breathing Problem = No: [9 Yes/0 No]\n",
      " |  |  |  |  |  |  |  Fatigue  = No: [23 Yes/0 No]\n",
      " |  |  Sore throat = No: [767 No/294 Yes]\n",
      " |  |  |  Breathing Problem = No: [640 No/51 Yes]\n",
      " |  |  |  |  Attended Large Gathering = No: [572 No/0 Yes]\n",
      " |  |  |  |  Attended Large Gathering = Yes: [68 No/51 Yes]\n",
      " |  |  |  |  |  Dry Cough = No: [68 No/1 Yes]\n",
      " |  |  |  |  |  |  Diabetes = No: [48 No/0 Yes]\n",
      " |  |  |  |  |  |  Diabetes = Yes: [20 No/1 Yes]\n",
      " |  |  |  |  |  |  |  Headache = Yes: [15 No/0 Yes]\n",
      " |  |  |  |  |  |  |  Headache = No: [5 No/1 Yes]\n",
      " |  |  |  |  |  |  |  |  Chronic Lung Disease = No: [3 No/0 Yes]\n",
      " |  |  |  |  |  |  |  |  Chronic Lung Disease = Yes: [2 No/1 Yes]\n",
      " |  |  |  |  |  Dry Cough = Yes: [50 Yes/0 No]\n",
      " |  |  |  Breathing Problem = Yes: [243 Yes/127 No]\n",
      " |  |  |  |  Attended Large Gathering = No: [127 No/114 Yes]\n",
      " |  |  |  |  |  Fever = Yes: [114 Yes/89 No]\n",
      " |  |  |  |  |  |  Dry Cough = Yes: [114 Yes/62 No]\n",
      " |  |  |  |  |  |  |  Contact with COVID Patient = No: [66 Yes/62 No]\n",
      " |  |  |  |  |  |  |  |  Running Nose = Yes: [45 No/28 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Chronic Lung Disease = No: [34 No/16 Yes]\n",
      " |  |  |  |  |  |  |  |  |  Chronic Lung Disease = Yes: [12 Yes/11 No]\n",
      " |  |  |  |  |  |  |  |  |  |  Heart Disease = Yes: [11 No/6 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Heart Disease = No: [6 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  Running Nose = No: [38 Yes/17 No]\n",
      " |  |  |  |  |  |  |  |  |  Heart Disease = Yes: [17 No/14 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Headache = No: [17 No/8 Yes]\n",
      " |  |  |  |  |  |  |  |  |  |  Headache = Yes: [6 Yes/0 No]\n",
      " |  |  |  |  |  |  |  |  |  Heart Disease = No: [24 Yes/0 No]\n",
      " |  |  |  |  |  |  |  Contact with COVID Patient = Yes: [48 Yes/0 No]\n",
      " |  |  |  |  |  |  Dry Cough = No: [27 No/0 Yes]\n",
      " |  |  |  |  |  Fever = No: [38 No/0 Yes]\n",
      " |  |  |  |  Attended Large Gathering = Yes: [129 Yes/0 No]\n",
      " |  Abroad travel = Yes: [2451 Yes/0 No]\n"
     ]
    }
   ],
   "source": [
    "root = Node(dataframe, 10)\n",
    "decision_tree = treeBuilder(root, 0, colAttributes, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now we have a decision tree of depth 10. This decision tree can be easily followed because the lines on the left side tell us the depth of the node we are looking at. Branches are indicated by indents and the number of lines tell us how deep we are into the tree. When you reach a line that has no more indents, you have reached a leaf node because one of the stopping criteria has been met. For example, on the fourth line down from the top, Attended Large Gathering = Yes: [977 Yes/0 No], although this does not have depth 10, it no longer indents and splits anymore because this data is perfectly classified since all the examples are 'Yes'. At the line that says, Fatigue  = Yes: [38 No/28 Yes], this no longer splits further because the max depth of 10 was reached. When reading this tree top to bottom and you reach a leaf node, we explore the opposite path based on the previous node. At each line, you have the attribute name and then after the equals sign, you will then have a 'Yes' or 'No' value. This indicates whether a person has the attribute or does not have the attribute. Then, we see a ratio of 'Yes' and 'No' responses in brackets. These numbers indicate the subset of people who have COVID, based on their value for that attribute. In order to make a prediction based on someone's attribute, you take the larger proportion of people from the brackets. So for example, if we had someone who traveled abroad, we would predict that they do have COVID because following the tree, we see that Abroad travel = Yes: [2451 Yes/0 No] and the majority of people who have traveled abroad have COVID and so we predict that future people will also have COVID if they travel abroad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we do with a decision tree? We can make predictions! So we will need to follow the branches on the tree based on our sample example and then come with up a prediction whether they have COVID or not. Let's implement the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(root, data):\n",
    "    pred = []\n",
    "    dummyRoot = root\n",
    "    for row in data:\n",
    "        for i in range(root.maxDepth):\n",
    "            if root.colIndex == None:\n",
    "                break\n",
    "            x = row[root.colIndex]\n",
    "            if x == root.left.data.iloc[0][root.colIndex]:\n",
    "                root = root.left\n",
    "            else:\n",
    "                root = root.right\n",
    "        (maj, majC, mino, minC) = majorityVote(root)\n",
    "        guess = maj\n",
    "        root = dummyRoot\n",
    "        pred += [guess]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we predict on our training data, when we have a decision tree of depth 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "root = Node(dataframe, 5)\n",
    "decision_tree = treeBuilder(root, 0, colAttributes, False)\n",
    "trainData = dataframe.values.tolist()\n",
    "trainPreds = predictions(root, trainData)\n",
    "print(trainPreds[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our output, we predict that the first 10 samples from our dataset all have COVID, following the decision tree with depth 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Rate and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making our predictions, it would be useful to see how well our model performed. We will look at the error rate and see what percentage of examples our model predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorRate(predictions, real):\n",
    "    total = 0\n",
    "    wrong = 0\n",
    "    for index in range(len(predictions)):\n",
    "        if predictions[index] != real[index]:\n",
    "            wrong += 1\n",
    "        total += 1\n",
    "    return (wrong / float(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04766286345233714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realLabels = dataframe['COVID-19']\n",
    "realLabels = realLabels.values.tolist()\n",
    "errorRate(trainPreds, realLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that when we train our decision tree to have a depth of 5, our error rate is about 4.76%! Let's play around and see what happens with the error rate as we change the depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 0 : error rate =  0.19341185130658814\n",
      "Depth = 4 : error rate =  0.0826278984173721\n",
      "Depth = 8 : error rate =  0.024475524475524476\n",
      "Depth = 12 : error rate =  0.01729849098270151\n",
      "Depth = 16 : error rate =  0.01729849098270151\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(colAttributes), 4):\n",
    "    root = Node(dataframe, i)\n",
    "    decision_tree = treeBuilder(root, 0, colAttributes, False)\n",
    "    trainData = dataframe.values.tolist()\n",
    "    trainPreds = predictions(root, trainData)\n",
    "    print('Depth = ' + str(i) + \" : error rate = \", errorRate(trainPreds, realLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, as the depth of our tree increases, the error rate decreases, telling us that our model is getting better on the training data. This should make sense because as we split on more attributes, we are getting more and more specific and we can correctly classify the training examples. However, this does not mean we should always make our decision tree have a max depth where we use all possible attributes. This will cause overfitting because although our decision tree is getting better at predicting our training examples, we have no idea how well it will do on unseen data. Our model could very well be fitting the training data, but the training data is not always an accurate picture of unseen data and so we don't want our model to be too specific for just our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to avoid overfitting your decision tree is called reduced error pruning. What you want to do is split your training data into a new training dataset as well as a validation set. It is typical to split 80%/20%. Then, using your new training set, train your tree so that it classifies the training dataset as best as possible (produces lowest possible error rate). Then using the validation set, we are going to prune the tree so that we produce the lowest error rate on the validation set. To do this, at each internal node, we consider turning it into a leaf node and pruning the tree below it. Then using this pruned tree, make predictions with the validation set and get the error rate. Keep repeating this process with every internal node, then choose the tree that gives the lowest validation error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have chosen the best depth for your tree, you can go ahead and run your model on some new unseen data (test data) and see how accurately your model makes predictions. However, after running your tree on test data, you should not go back and revise your model based on the test data. Doing so would harm the integrity of your original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial outlined the general algorithm for building a classification decision tree in Python and using a real world example to highlight the usefulness and benefits to the decision tree model. If you would like to learn more about the decision tree algorithm or look for more datasets to try implementing your own, here are some useful links.\n",
    "\n",
    "1. COVID dataset: https://www.kaggle.com/hemanthhari/symptoms-and-covid-presence\n",
    "2. Decision Trees in Machine Learning: https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
    "3. Pandas Library: https://pandas.pydata.org/\n",
    "4. General Notes about Decision Trees: http://www.cs.cmu.edu/~mgormley/courses/10601/slides/lecture2-dtrees-overfit.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
